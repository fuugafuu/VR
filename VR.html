<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>HUD Object Detection AR</title>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>
  <script src="https://cdn.jsdelivr.net/npm/gsap@3"></script>
  <script src="https://aframe.io/releases/1.3.0/aframe.min.js"></script>
  <style>
    body, html { margin: 0; padding: 0; overflow: hidden; background: black; }
    canvas { position: absolute; top: 0; left: 0; z-index: 1; }
    #hud { position: absolute; top: 0; left: 0; z-index: 2; pointer-events: none; }
  </style>
</head>
<body>
  <video id="video" width="640" height="480" autoplay muted playsinline style="display:none;"></video>
  <canvas id="canvas" width="640" height="480"></canvas>
  <div id="hud"></div>

  <a-scene embedded arjs>
    <a-entity id="compass" position="0 1 -3">
      <a-text value="N" color="red" position="0 0 0"></a-text>
    </a-entity>
  </a-scene>

  <script>
    const video = document.getElementById('video');
    const canvas = document.getElementById('canvas');
    const ctx = canvas.getContext('2d');
    const hud = document.getElementById('hud');

    async function setupCamera() {
      const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: 'environment' }, audio: false });
      video.srcObject = stream;
      return new Promise(resolve => {
        video.onloadedmetadata = () => resolve(video);
      });
    }

    async function run() {
      await setupCamera();
      const model = await cocoSsd.load();
      detectFrame(video, model);
    }

    function detectFrame(video, model) {
      model.detect(video).then(predictions => {
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
        predictions.forEach(pred => {
          const [x, y, width, height] = pred.bbox;
          ctx.strokeStyle = 'red';
          ctx.lineWidth = 2;
          ctx.strokeRect(x, y, width, height);
          ctx.font = '16px sans-serif';
          ctx.fillStyle = 'red';
          ctx.fillText(pred.class, x, y > 10 ? y - 5 : 10);

          speak(pred.class);

          // HUDアニメーション
          const tag = document.createElement('div');
          tag.innerText = pred.class;
          tag.style.position = 'absolute';
          tag.style.left = `${x}px`;
          tag.style.top = `${y}px`;
          tag.style.color = 'lime';
          tag.style.fontSize = '20px';
          hud.appendChild(tag);

          gsap.to(tag, { opacity: 0, y: -20, duration: 2, onComplete: () => hud.removeChild(tag) });
        });
        requestAnimationFrame(() => detectFrame(video, model));
      });
    }

    function speak(text) {
      if (!window.speaking) {
        window.speaking = true;
        const utterance = new SpeechSynthesisUtterance(text);
        utterance.lang = 'ja-JP';
        utterance.onend = () => window.speaking = false;
        speechSynthesis.speak(utterance);
      }
    }

    run();
  </script>
</body>
</html>
